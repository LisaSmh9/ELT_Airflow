# Projet ELT avec Apache Airflow â€“ PrÃ©diction de la consommation Ã©nergÃ©tique

## ğŸ¯ Objectif du projet
Ce projet a pour objectif de mettre en place une pipeline ELT (Extract â†’ Load â†’ Transform)  orchestrÃ©e avec Apache Airflow, pour collecter, stocker et transformer des donnÃ©es nÃ©cessaires Ã  la prÃ©diction de la consommation Ã©nergÃ©tique (coefficient des profils).
Il permet de collecter automatiquement les donnÃ©es suivantes :
- Les tempÃ©ratures normales et rÃ©alisÃ©es (depuis une API publique)
- Les jours fÃ©riÃ©s et vacances scolaires en France
- Les coefficients de profils (depuis un fichier Parquet)
Ces donnÃ©es sont chargÃ©es dans **PostgreSQL (bronze)**, puis transformÃ©es et stockÃ©es dans **DuckDB (gold)**.

## ğŸš€ Installation et exÃ©cution avec Docker

### ğŸ“¥ 1. TÃ©lÃ©charger et extraire le dossier atelier 
- TÃ©lÃ©chargez l'archive `atelier.zip` du projet et **dÃ©compressez-la**.
- AccÃ©dez au dossier **atelier** dans le terminal :
```bash
  cd atelier
```

### ğŸ› ï¸ 2. Construire lâ€™image Docker
ExÃ©cutez la commande suivante pour construire lâ€™image Docker :
```bash
docker build -t airflow_custom .
```
Cette commande utilise le **Dockerfile** prÃ©sent dans le dossier pour crÃ©er une image Docker nommÃ©e **airflow_custom**.

#### VÃ©rifier la crÃ©ation de lâ€™image :
Une fois la construction terminÃ©e, listez les images Docker disponibles :
```bash
docker images
```
Vous devriez voir une image nommÃ©e airflow_custom dans la liste.

### ğŸš€ 3. DÃ©marrer Apache Airflow avec Docker Compose
Lancez tous les services avec :
```bash
docker-compose up -d
```
Cette commande dÃ©marre Apache Airflow ainsi que ses dÃ©pendances (PostgreSQL, DuckDB...).

### ğŸŒ 4. AccÃ©der Ã  l'interface Airflow
- Ouvrez un navigateur et allez sur http://localhost:8080.
- Connectez-vous avec :
     - **Utilisateur** : airflow
     - **Mot de passe** : airflow


## ğŸ§© Structure du projet
atelier3/
â”œâ”€â”€ dags/                        â†’ Contient les DAGs Airflow dÃ©finissant les Ã©tapes du pipeline :
â”‚   â”œâ”€â”€ dag_vacancesscolaires.py     â†’ Extraction des vacances scolaires et jours fÃ©riÃ©s
â”‚   â”œâ”€â”€ elt_temperature_pipeline.py  â†’ Pipeline dâ€™extraction des tempÃ©ratures
â”‚   â”œâ”€â”€ ingest_parquet_with_hook.py  â†’ Ingestion des coefficients de profils (parquet)
â”‚   â”œâ”€â”€ create_gold_table.py         â†’ CrÃ©ation de la table finale dans DuckDB (gold)
â”‚   â”œâ”€â”€ dag_full_refresh.py          â†’ TÃ¢che de suppression des tables (full refresh)
â”‚   â””â”€â”€ main_dag.py                  â†’ (Facultatif ou global) Coordination possible des DAGs
â”‚
â”œâ”€â”€ tests/                      â†’ Tests unitaires pour valider chaque composant du pipeline :
â”‚   â”œâ”€â”€ test_dag_vacancesscolaires.py
â”‚   â”œâ”€â”€ test_elt_temperature_pipeline.py
â”‚   â”œâ”€â”€ test_ingest_parquet.py
â”‚   â””â”€â”€ test_gold_table.py
â”‚
â”œâ”€â”€ CDP/                        â†’ Dossier contenant le fichier `coefficients-des-profils.parquet`
â”‚
â”œâ”€â”€ docker/                     â†’ Configuration Docker additionnelle (notamment DuckDB)
â”‚
â”œâ”€â”€ logs/                       â†’ Dossier de stockage des logs gÃ©nÃ©rÃ©s par Airflow
â”‚
â”œâ”€â”€ Dockerfile                  â†’ Image personnalisÃ©e dâ€™Apache Airflow avec les dÃ©pendances
â”œâ”€â”€ docker-compose.yml          â†’ Orchestration de tous les services (Airflow, PostgreSQL, DuckDB...)
â”œâ”€â”€ requirements.txt            â†’ DÃ©pendances Python pour le projet
â””â”€â”€ README.md                   â†’ Documentation du projet (ce fichier)




