# Projet ELT avec Apache Airflow â€“ PrÃ©diction de la consommation Ã©nergÃ©tique

## ğŸ¯ Objectif du projet
Ce projet a pour objectif de mettre en place une pipeline ELT (Extract â†’ Load â†’ Transform)  orchestrÃ©e avec Apache Airflow, pour collecter, stocker et transformer des donnÃ©es nÃ©cessaires Ã  la prÃ©diction de la consommation Ã©nergÃ©tique (coefficient des profils).
Il permet de collecter automatiquement les donnÃ©es suivantes :
- Les tempÃ©ratures normales et rÃ©alisÃ©es (depuis une API publique)
- Les jours fÃ©riÃ©s et vacances scolaires en France
- Les coefficients de profils (depuis un fichier Parquet)
Ces donnÃ©es sont chargÃ©es dans **PostgreSQL (bronze)**, puis transformÃ©es et stockÃ©es dans **DuckDB (gold)**.

## ğŸš€ Lancement du projet
```bash
git clone https://github.com/LisaSmh9/ELT_Airflow.git
cd atelier3
```
## ğŸ§© Structure du projet
```bash
atelier3/
â”œâ”€â”€ dags/                      # Contient les DAGs Airflow dÃ©finissant les Ã©tapes du pipeline
â”‚   â”œâ”€â”€ dag_vacancesscolaires.py     # Extraction des vacances scolaires et jours fÃ©riÃ©s
â”‚   â”œâ”€â”€ elt_temperature_pipeline.py  # Pipeline dâ€™extraction des tempÃ©ratures
â”‚   â”œâ”€â”€ ingest_parquet_with_hook.py  # Ingestion des coefficients de profils (parquet)
â”‚   â”œâ”€â”€ create_gold_table.py         # CrÃ©ation de la table finale dans DuckDB (gold)
â”‚   â”œâ”€â”€ dag_full_refresh.py          # TÃ¢che de suppression des tables (full refresh)
â”‚   â””â”€â”€ main_dag.py                  
â”‚
â”œâ”€â”€ tests/                     # Tests unitaires pour valider chaque composant du pipeline
â”‚   â”œâ”€â”€ test_dag_vacancesscolaires.py
â”‚   â”œâ”€â”€ test_elt_temperature_pipeline.py
â”‚   â”œâ”€â”€ test_ingest_parquet.py
â”‚   â””â”€â”€ test_gold_table.py
â”‚
â”œâ”€â”€ CDP/                       # Contient le fichier coefficients-des-profils.parquet
â”œâ”€â”€ docker/                    # Configuration Docker 
â”œâ”€â”€ logs/                      # Stockage des logs Airflow

â”œâ”€â”€ Dockerfile                 # Image Docker personnalisÃ©e pour Airflow
â”œâ”€â”€ docker-compose.yml         # DÃ©finit les services : Airflow, PostgreSQL, DuckDB...
requirements.txt               # DÃ©pendances Python
README.md                      # Documentation du projet 
```

## ğŸ³ Installation avec Docker


### ğŸ› ï¸ 1. Construire lâ€™image Docker
ExÃ©cutez la commande suivante pour construire lâ€™image Docker :
```bash
docker build -t airflow_custom .
```
Cette commande utilise le **Dockerfile** prÃ©sent dans le dossier pour crÃ©er une image Docker nommÃ©e **airflow_custom**.

#### VÃ©rifier la crÃ©ation de lâ€™image :
Une fois la construction terminÃ©e, listez les images Docker disponibles :
```bash
docker images
```
Vous devriez voir une image nommÃ©e airflow_custom dans la liste.

### ğŸš€ 2. DÃ©marrer Apache Airflow avec Docker Compose
Lancez tous les services avec :
```bash
docker-compose up -d
```
Cette commande dÃ©marre Apache Airflow ainsi que ses dÃ©pendances (PostgreSQL, DuckDB...).

### ğŸŒ 3. AccÃ©der Ã  l'interface Airflow
- Ouvrez un navigateur et allez sur http://localhost:8080.
- Connectez-vous avec :
     - **Utilisateur** : airflow
     - **Mot de passe** : airflow

## â›ï¸ Description des DAGs
Le projet est composÃ© de plusieurs **DAGs Apache Airflow**, chacun orchestrant une Ã©tape prÃ©cise du pipeline ELT :

| DAG | Description |
|-----|-------------|
| `dag_vacancesscolaires.py` | Extrait les jours fÃ©riÃ©s (librairie `holidays`) et vacances scolaires (librairie `vacances_scolaires_france`) pour les annÃ©es 2023 Ã  2025, puis les stocke dans la base PostgreSQL. |
| `elt_temperature_pipeline.py` | RÃ©cupÃ¨re les tempÃ©ratures rÃ©alisÃ©es et normales depuis lâ€™API dâ€™Enedis, jour par jour et remplit une table PostgreSQL `temperatures`. |
| `ingest_parquet_with_hook.py` | IngÃ¨re les coefficients de profils depuis un fichier `.parquet` (prÃ©sent dans le dossier `CDP/`), en effectuant un **UPSERT** dans PostgreSQL sur les clÃ©s `(horodate, sous_profil)`. |
| `create_gold_table.py` | Fusionne les trois sources (vacances, tempÃ©ratures, coefficients) pour crÃ©er une table finale propre dans **DuckDB** (`data_model_inputs`) avec toutes les transformations temporelles nÃ©cessaires. |
| `dag_full_refresh.py` | Supprime toutes les tables de la base (bronze et gold) si confirmÃ© manuellement, puis enchaÃ®ne automatiquement l'exÃ©cution de tous les DAGs mÃ©tiers. |

## ğŸ§ª Tests
### Structure des tests

| Test | Description |
|-----|-------------|
| `test_dag_vacancesscolaires.py` | VÃ©rifie que le DAG des vacances sâ€™importe correctement et que ses tÃ¢ches sont bien dÃ©finies. |
| `test_elt_temperature_pipeline.py` | Mocke les appels Ã  lâ€™API Enedis pour tester la rÃ©cupÃ©ration de tempÃ©ratures (succÃ¨s + Ã©chec HTTP). |
| `test_gold_table.py` | Valide la logique de transformation finale vers la table data_model_inputs de DuckDB. |
| `test_ingest_parquet.py` | Simule lâ€™ingestion de fichiers .parquet, vÃ©rifie le comportement dâ€™idempotence et la gestion des .done. |

### Lancer les tests
```bash
pytest tests/
```

## ğŸ§¯ Monitoring

Le projet intÃ¨gre un mÃ©canisme de monitoring via des callbacks dÃ©finis dans dags/utils/callbacks_modules.py.
Ces fonctions permettent de notifier automatiquement lâ€™Ã©quipe en cas de succÃ¨s ou dâ€™Ã©chec des tÃ¢ches critiques dans les DAGs.

ğŸ”” Notifications

**notify_success(context) :** appelÃ©e automatiquement lorsquâ€™une tÃ¢che se termine avec succÃ¨s.

**notify_failure(context) :** appelÃ©e lorsquâ€™une tÃ¢che Ã©choue.

## ğŸ” Mode Full Refresh

Le DAG **dag_full_refresh** permet un nettoyage total des donnÃ©es et une relance sÃ©quentielle de tous les DAGs du projet.

### âœ… Fonctionnement

Suppression des tables : temperatures, profil_coefficients, holidays, data_model_inputs

RecrÃ©ation des donnÃ©es depuis les sources (API, Librairies, Fichier)

Rechargement complet de la table finale dans DuckDB
```bash
âš ï¸ ATTENTION : OpÃ©ration irrÃ©versible
```


## ğŸ§° Stack technique

**Orchestration :** Apache Airflow (v2.9.1)

**Base de donnÃ©es :** PostgreSQL (bronze)

**Data Warehouse :** DuckDB (gold)

**Conteneurisation :** Docker, Docker Compose

**Langage :** Python

**Tests :** Pytest

**Librairies data :** pandas, sqlalchemy, holidays, vacances_scolaires_france


