# Projet ELT avec Apache Airflow â€“ PrÃ©diction de la consommation Ã©nergÃ©tique

## ğŸ¯ Objectif du projet
Ce projet a pour objectif de mettre en place une pipeline ELT (Extract â†’ Load â†’ Transform)  orchestrÃ©e avec Apache Airflow, pour collecter, stocker et transformer des donnÃ©es nÃ©cessaires Ã  la prÃ©diction de la consommation Ã©nergÃ©tique (coefficient des profils).
Il permet de collecter automatiquement les donnÃ©es suivantes :
- Les tempÃ©ratures normales et rÃ©alisÃ©es (depuis une API publique)
- Les jours fÃ©riÃ©s et vacances scolaires en France
- Les coefficients de profils (depuis un fichier Parquet)
Ces donnÃ©es sont chargÃ©es dans **PostgreSQL (bronze)**, puis transformÃ©es et stockÃ©es dans **DuckDB (gold)**.

## ğŸš€ Installation et exÃ©cution avec Docker

### ğŸ“¥ 1. TÃ©lÃ©charger et extraire le dossier atelier 
- TÃ©lÃ©chargez l'archive `atelier.zip` du projet et **dÃ©compressez-la**.
- AccÃ©dez au dossier **atelier** dans le terminal :
```bash
  cd atelier
```

### ğŸ› ï¸ 2. Construire lâ€™image Docker
ExÃ©cutez la commande suivante pour construire lâ€™image Docker :
```bash
docker build -t airflow_custom .
```
Cette commande utilise le **Dockerfile** prÃ©sent dans le dossier pour crÃ©er une image Docker nommÃ©e **airflow_custom**.

#### VÃ©rifier la crÃ©ation de lâ€™image :
Une fois la construction terminÃ©e, listez les images Docker disponibles :
```bash
docker images
```
Vous devriez voir une image nommÃ©e airflow_custom dans la liste.

### ğŸš€ 3. DÃ©marrer Apache Airflow avec Docker Compose
Lancez tous les services avec :
```bash
docker-compose up -d
```
Cette commande dÃ©marre Apache Airflow ainsi que ses dÃ©pendances (PostgreSQL, DuckDB...).

### ğŸŒ 4. AccÃ©der Ã  l'interface Airflow
- Ouvrez un navigateur et allez sur http://localhost:8080.
- Connectez-vous avec :
     - **Utilisateur** : airflow
     - **Mot de passe** : airflow


## ğŸ§© Structure du projet
```bash
atelier3/
â”œâ”€â”€ dags/                      # Contient les DAGs Airflow dÃ©finissant les Ã©tapes du pipeline
â”‚   â”œâ”€â”€ dag_vacancesscolaires.py     # Extraction des vacances scolaires et jours fÃ©riÃ©s
â”‚   â”œâ”€â”€ elt_temperature_pipeline.py  # Pipeline dâ€™extraction des tempÃ©ratures
â”‚   â”œâ”€â”€ ingest_parquet_with_hook.py  # Ingestion des coefficients de profils (parquet)
â”‚   â”œâ”€â”€ create_gold_table.py         # CrÃ©ation de la table finale dans DuckDB (gold)
â”‚   â”œâ”€â”€ dag_full_refresh.py          # TÃ¢che de suppression des tables (full refresh)
â”‚   â””â”€â”€ main_dag.py                  
â”‚
â”œâ”€â”€ tests/                     # Tests unitaires pour valider chaque composant du pipeline
â”‚   â”œâ”€â”€ test_dag_vacancesscolaires.py
â”‚   â”œâ”€â”€ test_elt_temperature_pipeline.py
â”‚   â”œâ”€â”€ test_ingest_parquet.py
â”‚   â””â”€â”€ test_gold_table.py
â”‚
â”œâ”€â”€ CDP/                       # Contient le fichier coefficients-des-profils.parquet
â”œâ”€â”€ docker/                    # Configuration Docker 
â”œâ”€â”€ logs/                      # Stockage des logs Airflow

â”œâ”€â”€ Dockerfile                 # Image Docker personnalisÃ©e pour Airflow
â”œâ”€â”€ docker-compose.yml         # DÃ©finit les services : Airflow, PostgreSQL, DuckDB...
requirements.txt               # DÃ©pendances Python
README.md                      # Documentation du projet 
```

